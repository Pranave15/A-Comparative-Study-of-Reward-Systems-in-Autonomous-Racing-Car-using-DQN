{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb2adf-64fb-4a9c-b9b0-087cbf20404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#success -ve\n",
    "import pygame\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63669e-9451-41bb-a23e-3e67e1fb7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(tf.keras.Model):\n",
    "    def __init__(self, action_space):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(action_space, activation='linear')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "    \n",
    "    def compile_model(self):\n",
    "        self.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc3ac5-f666-484b-96dd-9789c96ef2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_space, action_space, buffer_size=10000, batch_size=64, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "        self.model = DeepQNetwork(action_space)\n",
    "        self.model.compile_model()\n",
    "        self.target_model = DeepQNetwork(action_space)\n",
    "        self.target_model.compile_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        # Define model saving parameters\n",
    "        self.model_save_folder = \"C:/AMRITA/Sem6/RL/forward_negative\"\n",
    "        self.checkpoint_rewards = [100]  # Initial reward for the first checkpoint set to 100\n",
    "        self.next_checkpoint_reward_multiplier = 2  # Multiplier for next checkpoint reward\n",
    "        self.penalty_reward = -50  # Penalty for trying to backtrack\n",
    "\n",
    "        # Create the model saving directory if it does not exist\n",
    "        if not os.path.exists(self.model_save_folder):\n",
    "            os.makedirs(self.model_save_folder)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_space)\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.buffer.buffer) < self.batch_size:\n",
    "            return\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        target = self.model.predict(states)\n",
    "        target_next = self.target_model.predict(next_states)\n",
    "        for i in range(self.batch_size):\n",
    "            if dones[i]:\n",
    "                target[i][actions[i]] = rewards[i]\n",
    "            else:\n",
    "                target[i][actions[i]] = rewards[i] + self.gamma * np.amax(target_next[i])\n",
    "        self.model.fit(states, target, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save_model(self, episode):\n",
    "        if episode % 500 == 0:\n",
    "            model_save_path = os.path.join(self.model_save_folder, f'dqn_model_episode_{episode}.h5')\n",
    "            self.model.save(model_save_path)\n",
    "            print(f\"Model saved successfully for episode {episode} in {model_save_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4b3b72-a578-4bda-af3a-c0562df19af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutonomousCarsEnv:\n",
    "    def __init__(self, agent, track_size=(800, 600), num_cars=1, num_checkpoints=10, max_steps_per_episode=1000):\n",
    "        self.track_size = track_size\n",
    "        self.num_cars = num_cars\n",
    "        self.car_size = (40, 20)\n",
    "        self.forward_speed = 8\n",
    "        self.rotation_angle = 10\n",
    "        self.agent = agent\n",
    "    \n",
    "        # Initialize Pygame\n",
    "        pygame.init()\n",
    "        self.screen = pygame.display.set_mode(self.track_size)\n",
    "        pygame.display.set_caption(\"Autonomous Cars Environment\")\n",
    "    \n",
    "        # Load car image\n",
    "        self.car_image = pygame.image.load(\"C:\\AMRITA\\Sem6\\RL\\car.png\")\n",
    "        self.car_image = pygame.transform.scale(self.car_image, self.car_size)\n",
    "    \n",
    "        # Colors\n",
    "        self.background_color = (0, 255, 0)\n",
    "        self.track_color = (0, 0, 0)\n",
    "        self.start_line_color = (255, 0, 0)  # Changed start line color to red\n",
    "    \n",
    "        # Track parameters\n",
    "        self.track_width = self.car_size[0]\n",
    "        self.inner_semi_major_axis = 200\n",
    "        self.inner_semi_minor_axis = 100\n",
    "        self.outer_semi_major_axis = self.inner_semi_major_axis + self.track_width\n",
    "        self.outer_semi_minor_axis = self.inner_semi_minor_axis + self.track_width\n",
    "\n",
    "        # Create track vertices (elliptical loop)\n",
    "        self.track_inner_vertices, self.track_outer_vertices = self.create_track()  # Assign track vertices here\n",
    "    \n",
    "        # Start line position\n",
    "        self.start_line_pos = self.generate_start_line_point()\n",
    "    \n",
    "        # Checkpoints\n",
    "        self.num_checkpoints = num_checkpoints\n",
    "        self.checkpoints, self.checkpoint_rewards = self.generate_checkpoints()  # Assign checkpoints and rewards\n",
    "        self.current_checkpoint = 0\n",
    "    \n",
    "        # Initialize car position and angle\n",
    "        self.reset()\n",
    "    \n",
    "        # Font for rendering text\n",
    "        self.font = pygame.font.SysFont('Arial', 24)\n",
    "    \n",
    "        # Max steps per episode\n",
    "        self.max_steps_per_episode = max_steps_per_episode\n",
    "        self.current_step = 0\n",
    "        self.lap_count = 0  # Added lap count\n",
    "\n",
    "        # Create CSV file for logging\n",
    "        self.log_file = 'C:/AMRITA//Sem6/RL/negative_episode_logs.csv'\n",
    "        self.create_log_file()\n",
    "\n",
    "    def create_log_file(self):\n",
    "        if not os.path.isfile(self.log_file):\n",
    "            df = pd.DataFrame(columns=['Episode', 'Cumulative Reward', 'Distance Traveled', 'Lap Count'])\n",
    "            df.to_csv(self.log_file, index=False)\n",
    "\n",
    "    def log_episode(self, episode, cumulative_reward, distance_traveled):\n",
    "        df = pd.read_csv(self.log_file)\n",
    "        new_entry = {'Episode': episode, 'Cumulative Reward': cumulative_reward, 'Distance Traveled': distance_traveled, 'Lap Count': self.lap_count}\n",
    "        df = pd.concat([df, pd.DataFrame([new_entry])], ignore_index=True)\n",
    "        df.to_csv(self.log_file, index=False)\n",
    "\n",
    "    def generate_start_line_point(self):\n",
    "        angle = 0\n",
    "        radians = math.radians(angle)\n",
    "        center = (self.track_size[0] // 2, self.track_size[1] // 2)\n",
    "        start_x = center[0] + (self.inner_semi_major_axis + self.track_width / 2) * math.cos(radians)\n",
    "        start_y = center[1] + (self.inner_semi_minor_axis + self.track_width / 2) * math.sin(radians)\n",
    "        end_x = center[0] + (self.inner_semi_major_axis + self.track_width / 2) * math.cos(radians + math.pi)\n",
    "        end_y = center[1] + (self.inner_semi_minor_axis + self.track_width / 2) * math.sin(radians + math.pi)\n",
    "        return (start_x, start_y), (end_x, end_y)\n",
    "\n",
    "    def create_track(self):\n",
    "        track_inner_vertices = []\n",
    "        track_outer_vertices = []\n",
    "        center = (self.track_size[0] // 2, self.track_size[1] // 2)\n",
    "        for angle in range(0, 360, 5):\n",
    "            radians = math.radians(angle)\n",
    "            inner_x = center[0] + self.inner_semi_major_axis * math.cos(radians)\n",
    "            inner_y = center[1] + self.inner_semi_minor_axis * math.sin(radians)\n",
    "            outer_x = center[0] + self.outer_semi_major_axis * math.cos(radians)\n",
    "            outer_y = center[1] + self.outer_semi_minor_axis * math.sin(radians)\n",
    "            track_inner_vertices.append((inner_x, inner_y))\n",
    "            track_outer_vertices.append((outer_x, outer_y))\n",
    "        return track_inner_vertices, track_outer_vertices\n",
    "\n",
    "    def generate_checkpoints(self):\n",
    "        checkpoints = []\n",
    "        rewards = []\n",
    "        track_length = len(self.track_outer_vertices)\n",
    "        checkpoint_indices = np.linspace(0, track_length - 1, self.num_checkpoints, dtype=int)\n",
    "        for idx in checkpoint_indices:\n",
    "            checkpoint = self.track_outer_vertices[idx]\n",
    "            direction = np.array(checkpoint) - np.array(self.track_inner_vertices[idx])\n",
    "            scaled_direction = direction / np.linalg.norm(direction)\n",
    "            new_checkpoint = tuple(np.array(checkpoint) - scaled_direction * 20)  # Convert to tuple\n",
    "            checkpoints.append(new_checkpoint)\n",
    "            # Assign reward based on checkpoint index or any other logic\n",
    "            rewards.append(100 * (idx + 1))  # Example: increasing reward based on checkpoint index\n",
    "        return checkpoints, rewards\n",
    "    def step(self, action):\n",
    "        self.current_step += 1\n",
    "        old_pos = self.car_pos.copy()  # Keep track of the old position for distance calculation\n",
    "\n",
    "        if action == 0:  # Move forward and slightly right\n",
    "            self.car_pos[0] += self.forward_speed * math.cos(math.radians(self.car_angle))\n",
    "            self.car_pos[1] -= self.forward_speed * math.sin(math.radians(self.car_angle))\n",
    "            self.car_angle += self.rotation_angle / 2\n",
    "        elif action == 1:  # Move forward\n",
    "            self.car_pos[0] += self.forward_speed * math.cos(math.radians(self.car_angle))\n",
    "            self.car_pos[1] -= self.forward_speed * math.sin(math.radians(self.car_angle))\n",
    "        elif action == 2:  # Move forward and slightly left\n",
    "            self.car_pos[0] += self.forward_speed * math.cos(math.radians(self.car_angle))\n",
    "            self.car_pos[1] -= self.forward_speed * math.sin(math.radians(self.car_angle))\n",
    "            self.car_angle -= self.rotation_angle / 2\n",
    "\n",
    "        distance_traveled = np.linalg.norm(np.array(self.car_pos) - np.array(old_pos))\n",
    "    \n",
    "        reward = -1  # Small negative reward for each step\n",
    "\n",
    "        if not self.is_inside_track():\n",
    "            reward = -100  # Large negative reward for going off track\n",
    "            self.reset()\n",
    "            return reward, True, distance_traveled\n",
    "    \n",
    "        if self.reached_checkpoint():\n",
    "            reward = self.checkpoint_rewards[self.current_checkpoint]\n",
    "            self.current_checkpoint += 1\n",
    "            if self.current_checkpoint == len(self.checkpoints):\n",
    "                self.current_checkpoint = 0\n",
    "                self.lap_count += 1  # Increment lap count\n",
    "    \n",
    "        if self.current_step >= self.max_steps_per_episode:\n",
    "            self.reset()\n",
    "            return reward, True, distance_traveled\n",
    "    \n",
    "        return reward, False, distance_traveled\n",
    "\n",
    "    def is_inside_track(self):\n",
    "        center = (self.track_size[0] // 2, self.track_size[1] // 2)\n",
    "        car_rect = pygame.Rect(self.car_pos[0], self.car_pos[1], self.car_size[0], self.car_size[1])\n",
    "    \n",
    "        dx_inner = (car_rect.centerx - center[0]) ** 2 / self.inner_semi_major_axis ** 2\n",
    "        dy_inner = (car_rect.centery - center[1]) ** 2 / self.inner_semi_minor_axis ** 2\n",
    "        inside_inner_ellipse = dx_inner + dy_inner < 1\n",
    "    \n",
    "        dx_outer = (car_rect.centerx - center[0]) ** 2 / self.outer_semi_major_axis ** 2\n",
    "        dy_outer = (car_rect.centery - center[1]) ** 2 / self.outer_semi_minor_axis ** 2\n",
    "        inside_outer_ellipse = dx_outer + dy_outer < 1\n",
    "    \n",
    "        return inside_outer_ellipse and not inside_inner_ellipse\n",
    "\n",
    "    def reached_checkpoint(self):\n",
    "        checkpoint = self.checkpoints[self.current_checkpoint]\n",
    "        car_center = (self.car_pos[0], self.car_pos[1])\n",
    "        distance = np.linalg.norm(np.array(checkpoint) - np.array(car_center))\n",
    "        return distance < 20\n",
    "\n",
    "    def reset(self):\n",
    "        start_point = np.array(self.start_line_pos[0])\n",
    "        self.car_pos = start_point\n",
    "        self.car_angle = self.calculate_track_tangent_angle()\n",
    "        self.current_step = 0\n",
    "        self.current_checkpoint = 0\n",
    "        self.lap_count = 0  # Reset lap count\n",
    "\n",
    "    def calculate_track_tangent_angle(self):\n",
    "        nearest_vertex = self.find_nearest_track_vertex()\n",
    "        next_vertex = self.track_outer_vertices[(self.track_outer_vertices.index(nearest_vertex) + 1) % len(self.track_outer_vertices)]\n",
    "        tangent_angle = math.atan2(next_vertex[1] - nearest_vertex[1], next_vertex[0] - nearest_vertex[0]) * 180 / math.pi\n",
    "        return tangent_angle\n",
    "\n",
    "    def find_nearest_track_vertex(self):\n",
    "        min_distance = float('inf')\n",
    "        nearest_vertex = None\n",
    "        for vertex in self.track_outer_vertices:\n",
    "            distance = np.linalg.norm(np.array(vertex) - self.car_pos)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                nearest_vertex = vertex\n",
    "        return nearest_vertex\n",
    "\n",
    "    def render_text(self, text, position):\n",
    "        text_surface = self.font.render(text, True, (0, 0, 0))\n",
    "        self.screen.blit(text_surface, position)\n",
    "\n",
    "    def render(self, episode, cumulative_reward, total_distance_traveled):\n",
    "        self.screen.fill(self.background_color)\n",
    "    \n",
    "        pygame.draw.polygon(self.screen, self.track_color, self.track_outer_vertices, 0)\n",
    "        pygame.draw.polygon(self.screen, self.background_color, self.track_inner_vertices, 0)\n",
    "    \n",
    "        pygame.draw.circle(self.screen, (255, 0, 0), (int(self.start_line_pos[0][0]), int(self.start_line_pos[0][1])), 5)\n",
    "    \n",
    "        for checkpoint in self.checkpoints:\n",
    "            pygame.draw.circle(self.screen, (255, 255, 255), (int(checkpoint[0]), int(checkpoint[1])), 5)\n",
    "    \n",
    "        rotated_car = pygame.transform.rotate(self.car_image, self.car_angle)\n",
    "        car_rect = rotated_car.get_rect(center=(self.car_pos[0], self.car_pos[1]))\n",
    "        self.screen.blit(rotated_car, car_rect.topleft)\n",
    "    \n",
    "        self.render_text(f\"Episode: {episode}\", (10, 10))\n",
    "        self.render_text(f\"Cumulative Reward: {cumulative_reward}\", (10, 40))\n",
    "        self.render_text(f\"Distance Traveled: {total_distance_traveled}\", (10, 70))  # Display distance traveled\n",
    "        self.render_text(f\"Lap Count: {self.lap_count}\", (10, 100))  # Added lap count rendering\n",
    "    \n",
    "        pygame.display.flip()\n",
    "\n",
    "    def close(self):\n",
    "        pygame.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1829e6f-5503-495e-8aa9-7aea9b7fe106",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    state_space = 6  # Increased state space to include more features\n",
    "    action_space = 3\n",
    "    agent = DQNAgent(state_space, action_space)\n",
    "    env = AutonomousCarsEnv(agent, max_steps_per_episode=1000)\n",
    "\n",
    "    running = True\n",
    "    episode = 0\n",
    "    cumulative_reward = 0\n",
    "    total_distance_traveled = 0  # Track total distance traveled\n",
    "    episode_data = []  # To store episode metrics\n",
    "    \n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "    \n",
    "        car_angle_rad = math.radians(env.car_angle)\n",
    "        state = np.array([env.car_pos[0], env.car_pos[1], env.car_angle, np.cos(car_angle_rad), np.sin(car_angle_rad), env.current_checkpoint])\n",
    "        action = agent.choose_action(state.reshape(1, -1))\n",
    "        reward, done, distance_traveled = env.step(action)\n",
    "        car_angle_rad = math.radians(env.car_angle)\n",
    "        next_state = np.array([env.car_pos[0], env.car_pos[1], env.car_angle, np.cos(car_angle_rad), np.sin(car_angle_rad), env.current_checkpoint])\n",
    "        agent.remember(state, action, reward, next_state.reshape(1, -1), done)\n",
    "        agent.replay()\n",
    "    \n",
    "        cumulative_reward += reward\n",
    "        total_distance_traveled += distance_traveled  # Update total distance traveled\n",
    "        env.render(episode, cumulative_reward, total_distance_traveled)\n",
    "    \n",
    "        if done:\n",
    "            episode += 1\n",
    "            print(f\"Episode: {episode}, Distance Traveled: {total_distance_traveled}, Cumulative Reward: {cumulative_reward}, Lap Count: {env.lap_count}\")\n",
    "            agent.save_model(episode)  # Pass episode number to save_model\n",
    "            agent.update_target_model()\n",
    "            # Save model every 500 episodes or at the end of training\n",
    "            if episode % 500 == 0:\n",
    "                agent.save_model(episode)\n",
    "            \n",
    "            # Log episode data\n",
    "            env.log_episode(episode, cumulative_reward, total_distance_traveled)\n",
    "            cumulative_reward = 0\n",
    "            total_distance_traveled = 0  # Reset total distance traveled\n",
    "               \n",
    "    env.close()\n",
    "    \n",
    "    # Display metrics in Jupyter Notebook\n",
    "    episode_df = pd.read_csv('C:/AMRITA/Sem6/RL/negative_episode_logs.csv')\n",
    "    print(episode_df)\n",
    "    episode_df.plot(x='Episode', y=['Distance Traveled', 'Cumulative Reward', 'Lap Count'], kind='line', subplots=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
